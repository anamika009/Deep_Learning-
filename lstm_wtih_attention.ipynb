{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283a34c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:22:17.148803Z",
     "iopub.status.busy": "2024-04-05T08:22:17.148019Z",
     "iopub.status.idle": "2024-04-05T08:22:19.024966Z",
     "shell.execute_reply": "2024-04-05T08:22:19.023938Z"
    },
    "papermill": {
     "duration": 1.884264,
     "end_time": "2024-04-05T08:22:19.027062",
     "exception": false,
     "start_time": "2024-04-05T08:22:17.142798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data_path = \"G:/PARAG_S/A1_part2/data/data/train.json\"\n",
    "test_data_path = \"G:/PARAG_S/A1_part2/data/data/test.json\"\n",
    "val_data_path = \"G:/PARAG_S/A1_part2/data/data/dev.json\"\n",
    "glove_path = \"G:/NLP_assignment_2/glove.6B.100d.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2ba432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:22:19.036027Z",
     "iopub.status.busy": "2024-04-05T08:22:19.035385Z",
     "iopub.status.idle": "2024-04-05T08:23:00.963955Z",
     "shell.execute_reply": "2024-04-05T08:23:00.962877Z"
    },
    "papermill": {
     "duration": 41.935976,
     "end_time": "2024-04-05T08:23:00.966332",
     "exception": false,
     "start_time": "2024-04-05T08:22:19.030356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc dim : 256 | dec dim : 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANAND\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "c:\\Users\\ANAND\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Loading  training_data from JSON file\n",
    "with open(train_data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "#loading validation_data from json file\n",
    "with open(val_data_path, 'r') as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "#loading test_data from json file\n",
    "with open(test_data_path, 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "# Iterate through each problem\n",
    "for problem_data in data:\n",
    "    problem = problem_data[\"Problem\"]\n",
    "    linear_formula = problem_data[\"linear_formula\"]\n",
    "    answer = problem_data[\"answer\"]\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# def custom_tokenize_formula(formula):\n",
    "#     # Ensure spaces around | and inside parentheses\n",
    "#     formula = re.sub(r'(\\|)', r' \\1 ', formula)\n",
    "#     formula = re.sub(r'(\\()', r' \\1 ', formula)\n",
    "#     formula = re.sub(r'(\\))', r' \\1 ', formula)\n",
    "\n",
    "#     # Now use word_tokenize\n",
    "#     tokens = word_tokenize(formula)\n",
    "\n",
    "#     return tokens\n",
    "\n",
    "# def tokenize_formula(formula):\n",
    "#     # Define a regular expression pattern to match tokens\n",
    "#     pattern = r'\\||\\(|\\)|,|n\\d+|const_\\d+|\\w+'\n",
    "#     # Find all matches of the pattern in the input formula\n",
    "#     tokens = re.findall(pattern, formula)\n",
    "#     return tokens\n",
    "\n",
    "def tokenize_formula(formula):\n",
    "    # Split the formula by '|'\n",
    "    parts = formula.split('|')\n",
    "\n",
    "    # Tokenize each part\n",
    "    tokens = ['<sos>']\n",
    "    for part in parts:\n",
    "        if part:  \n",
    "            \n",
    "            open_parenthesis_index = part.find('(')\n",
    "            if open_parenthesis_index != -1:\n",
    "                function = part[:open_parenthesis_index]\n",
    "                arguments = part[open_parenthesis_index+1:-1]  \n",
    "                arguments_tokens = arguments.split(',')\n",
    "                # print(arguments_tokens)\n",
    "                arguments_token_new=[]\n",
    "                for tok in arguments_tokens:\n",
    "                    arguments_token_new.append(tok)\n",
    "                    arguments_token_new.append(',')\n",
    "                    \n",
    "                tokens.extend([function, '('] + arguments_token_new[:-1] + [')'])\n",
    "            else:\n",
    "                tokens.append(part)\n",
    "        if tokens[-1] != '|':\n",
    "            tokens.append('|')\n",
    "    tokens.append(\"<eos>\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# def tokenize_data(fromula):\n",
    "#     tokenized_data = []\n",
    "#     for item in data:\n",
    "#         # Tokenize the problem text\n",
    "#         # problem_tokens = word_tokenize(item['Problem'].lower())\n",
    "\n",
    "#         # Tokenize the formula using the updated method\n",
    "#         formula = item['formula'].lower()\n",
    "#         formula_tokens = ['<sos>'] + tokenize_formula(formula) + ['<eos>']\n",
    "\n",
    "#         # Get the answer\n",
    "#         # answer = item['answer']\n",
    "\n",
    "#         tokenized_data.append((formula_tokens))\n",
    "#     return tokenized_data\n",
    "\n",
    "\n",
    "\n",
    "# tokenized_data = tokenize_data(data)\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def build_vocab(texts, is_formula=False):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        if is_formula:\n",
    "            tokens = tokenize_formula(text.lower()) \n",
    "        else:\n",
    "            tokens = word_tokenize(text.lower())  \n",
    "        counter.update(tokens)\n",
    "\n",
    "    # Starting the vocab with special tokens\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1 , \"<sos>\":2, \"<eos>\":3 ,'<digit>':4}\n",
    "\n",
    "    \n",
    "    for word in counter:\n",
    "        if word.isdigit():\n",
    "          continue\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, counter\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "problem_texts = [item[\"Problem\"].lower() for item in data]\n",
    "linear_formulas = [item[\"linear_formula\"].lower() for item in data]\n",
    "\n",
    "# Build vocabularies\n",
    "problem_vocab, _ = build_vocab(problem_texts)\n",
    "formula_vocab, _ = build_vocab(linear_formulas, is_formula=True)\n",
    "\n",
    "\n",
    "# # Adding special tokens\n",
    "# for vocab in (problem_vocab, formula_vocab):\n",
    "#     vocab[\"<pad>\"] = 0\n",
    "#     vocab[\"<unk>\"] = 1\n",
    "\n",
    "# problem_vocab['<eos>'] = len(problem_vocab)\n",
    "\n",
    "\n",
    "# formula_vocab = {key: value for key, value in formula_vocab.items() if not key.isdigit()}\n",
    "# problem_vocab = {key: value for key, value in problem_vocab.items() if not key.isdigit()}\n",
    "\n",
    "# def reindex_vocab(vocab):\n",
    "    # \"\"\"Reindex the vocabulary.\"\"\"\n",
    "    # new_vocab = {}\n",
    "    # new_index = 0\n",
    "    # for key in vocab.keys():\n",
    "    #     new_vocab[key] = new_index\n",
    "    #     new_index += 1\n",
    "    # return new_vocab\n",
    "\n",
    "# Reindex vocabularies after filtering out digit keys\n",
    "# problem_vocab = reindex_vocab(problem_vocab)\n",
    "# formula_vocab = reindex_vocab(formula_vocab)\n",
    "\n",
    "# problem_vocab[\"<digit>\"] = len(problem_vocab)\n",
    "\n",
    "# def text_to_indices(text, vocab):\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "# tokenized_problems = [text_to_indices(problem, problem_vocab) for problem in problem_texts]\n",
    "# tokenized_formulas = [text_to_indices(formula, formula_vocab) for formula in linear_formulas]\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "glove_path = glove_path # Update this path\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "embedding_dim = 100\n",
    "# embedding_matrix = np.zeros((len(problem_vocab)-1, embedding_dim))\n",
    "\n",
    "# for word, idx in problem_vocab.items():\n",
    "#     embedding_vector = glove_embeddings.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_glove_embeddings(glove_file, word_to_index, embedding_dim=50):\n",
    "#     embeddings = {}\n",
    "#     with open(glove_file, 'r', encoding='utf8') as file:\n",
    "#         for line in file:\n",
    "#             parts = line.split()\n",
    "#             word = parts[0]\n",
    "#             vector = np.array(parts[1:], dtype=np.float32)\n",
    "#             embeddings[word] = vector\n",
    "\n",
    "#     # The +1 here ensures there is a row in the matrix for every index in word_to_index\n",
    "#     embedding_matrix = np.zeros((len(word_to_index)-1, embedding_dim))\n",
    "#     for word, index in word_to_index.items():\n",
    "#         if word in embeddings:\n",
    "#             embedding_matrix[index] = embeddings[word]\n",
    "#         else:\n",
    "#             # It's common to initialize embeddings for unknown words with zeros or small random numbers\n",
    "#             embedding_matrix[index] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "\n",
    "#     return embedding_matrix\n",
    "\n",
    "\n",
    "# Adjust the path to your GloVe file and the embedding dimension you wish to use\n",
    "# glove_path = \"G:/NLP_assignment_2/glove.6B.100d.txt\"\n",
    "# embedding_matrix = load_glove_embeddings(glove_path, vocab, 50)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "def add_digit_and_symbol_tokens_to_vocab(vocab):\n",
    "    for digit in map(str, range(10)):  # Digits 0-9\n",
    "        if digit not in vocab:\n",
    "            vocab[digit] = len(vocab)\n",
    "    for symbol in ['.', ',']:  \n",
    "        if symbol not in vocab:\n",
    "            vocab[symbol] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "problem_vocab = add_digit_and_symbol_tokens_to_vocab(problem_vocab)\n",
    "\n",
    "\n",
    "\n",
    "def create_embedding_matrix(vocab, embeddings, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in embeddings:\n",
    "            embedding_matrix[idx] = embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.rand(embedding_dim)  \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "problem_embedding_matrix = create_embedding_matrix(problem_vocab, glove_embeddings, embedding_dim)\n",
    "\n",
    "\n",
    "def text_to_indices(text, vocab):\n",
    "    tokens = ['<sos>'] + word_tokenize(text.lower()) + ['<eos>']   # Tokenize the text\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token.isdigit():  # Check if the token is a digit\n",
    "            indices.append(vocab.get(\"<digit>\", vocab[\"<unk>\"]))  # Use <digit> token\n",
    "        else:\n",
    "            indices.append(vocab.get(token, vocab[\"<unk>\"]))  # Use the token itself\n",
    "    return indices\n",
    "# def text_to_indices(text, vocab):\n",
    "#     # Custom tokenization to handle numbers, decimals, commas, and other punctuation separately\n",
    "#     tokens = ['<sos>']\n",
    "#     # Match decimal numbers, integers, words, and specific punctuation characters\n",
    "#     pattern = r'\\d+\\.\\d+|\\d+|[\\w]+|[.,!?;%]'\n",
    "#     for token in re.findall(pattern, text.lower()):\n",
    "#         if re.match(r'^\\d+\\.\\d+$', token):  # Decimal number\n",
    "#             # Break down decimal numbers into individual characters, keeping the decimal point\n",
    "#             tokens.extend([char for char in token])\n",
    "#         elif token.isdigit():  # Integer\n",
    "#             # Break down integers into individual digits\n",
    "#             tokens.extend([char for char in token])\n",
    "#         else:\n",
    "#             # For non-numeric tokens, add them directly\n",
    "#             tokens.append(token)\n",
    "#     tokens.append('<eos>')\n",
    "#     # print(tokens)\n",
    "#     # Convert tokens to indices, using '<unk>' for unknown tokens\n",
    "#     indices = [vocab.get(token, vocab.get('<unk>', 0)) for token in tokens]\n",
    "\n",
    "    # return indices\n",
    "# def tokenize_number(token, vocab):\n",
    "#     \"\"\"\n",
    "#     Breaks down a number into its constituent digits and symbols (., ,).\n",
    "#     Returns a list of indices corresponding to each digit/symbol in the vocab.\n",
    "#     \"\"\"\n",
    "#     # Symbols to be tokenized separately\n",
    "#     special_symbols = {'.', ','}\n",
    "#     indices = []\n",
    "#     for char in token:\n",
    "#         if char in special_symbols or char.isdigit():\n",
    "#             indices.append(vocab.get(char, vocab[\"<unk>\"]))\n",
    "#         else:\n",
    "#             raise ValueError(\"Unexpected character in number: {}\".format(char))\n",
    "#     return indices\n",
    "\n",
    "# def text_to_indices(text, vocab):\n",
    "#     tokens = ['<sos>'] + word_tokenize(text.lower()) + ['<eos>']\n",
    "#     indices = []\n",
    "#     for token in tokens:\n",
    "#         if any(char.isdigit() for char in token):  # If token contains any digit\n",
    "#             indices.extend(tokenize_number(token, vocab))\n",
    "#         else:\n",
    "#             indices.append(vocab.get(token, vocab[\"<unk>\"]))\n",
    "#     return indices\n",
    "\n",
    "\n",
    "\n",
    "# def text_to_indices(text, vocab):\n",
    "#     return [vocab.get(token, vocab[\"<unk>\"]) for token in word_tokenize(text.lower()) if not token.isdigit()]\n",
    "def text_to_indices_formula(text, vocab):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokenize_formula(text.lower())]\n",
    "# def text_to_indices(text, vocab):\n",
    "#     for token in word_tokenize(text.lower()):\n",
    "#         if not token.isdigit():\n",
    "#             return [vocab.get(token, vocab[\"<unk>\"])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized_problems = [text_to_indices(item[\"Problem\"], problem_vocab) for item in data]\n",
    "tokenized_formulas = [text_to_indices_formula(item[\"linear_formula\"], formula_vocab) for item in data]\n",
    "\n",
    "#tokenizing validation data\n",
    "\n",
    "tokenized_val_problems = [text_to_indices(item[\"Problem\"], problem_vocab) for item in val_data]\n",
    "tokenized_val_formulas = [text_to_indices_formula(item[\"linear_formula\"], formula_vocab) for item in val_data]\n",
    "\n",
    "\n",
    "#tokenizing test data\n",
    "tokenized_test_problems = [text_to_indices(item[\"Problem\"], problem_vocab) for item in test_data]\n",
    "tokenized_test_formulas = [text_to_indices_formula(item[\"linear_formula\"], formula_vocab) for item in test_data]\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class MathProblemDataset(Dataset):\n",
    "    def __init__(self, problems, formulas):\n",
    "        self.problems = problems\n",
    "        self.formulas = formulas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.problems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = torch.tensor(self.problems[idx], dtype=torch.long)\n",
    "        formula = torch.tensor(self.formulas[idx], dtype=torch.long)\n",
    "        return {\"problem\": problem, \"formula\": formula}\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    problems = [item[\"problem\"] for item in batch]\n",
    "    formulas = [item[\"formula\"] for item in batch]\n",
    "\n",
    "    problems_padded = pad_sequence(problems, batch_first=True, padding_value=problem_vocab[\"<pad>\"])\n",
    "    formulas_padded = pad_sequence(formulas, batch_first=True, padding_value=formula_vocab[\"<pad>\"])\n",
    "\n",
    "    return {\"problem\": problems_padded, \"formula\": formulas_padded}\n",
    "\n",
    "# Initializing the dataset with tokenized and indexed problems and formulas\n",
    "\n",
    "dataset = MathProblemDataset(tokenized_problems, tokenized_formulas)\n",
    "\n",
    "val_dataset = MathProblemDataset(tokenized_val_problems, tokenized_val_formulas)\n",
    "\n",
    "test_dataset = MathProblemDataset(tokenized_test_problems, tokenized_test_formulas)\n",
    "\n",
    "\n",
    "# Creating DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=20 ,shuffle=True, collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=20, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=20, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        print(f'enc dim : {enc_hid_dim} | dec dim : {dec_hid_dim}')\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # print(f'inside attention hidden : {hidden.shape} | encoder_outputs : {encoder_outputs.shape}')\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        # print(f'inside_attention src_len {src_len}')\n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # print(f'inside attention hidden {hidden.shape}')\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # print(f'inside_attention endoder ouput {encoder_outputs.shape}')\n",
    "\n",
    "        # print(f'concat {torch.cat((hidden, encoder_outputs), dim=2).shape}')\n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        # attention: [batch_size, src_len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, embedding_matrix, n_layers=1):\n",
    "        super().__init__()\n",
    "        # print(embedding_matrix.shape)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float),freeze=False)\n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, num_layers=n_layers, bidirectional=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # print(\"******************encoder**********************\")\n",
    "        # src shape: [src_len, batch_size]\n",
    "        # print(f'inside encoder src shape {src.shape}')\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [src_len, batch_size, emb_dim]\n",
    "        # print(f'inside encoder embedded shape {embedded.shape}')\n",
    "        # embedded_ = embedded.permute(1,0,2)\n",
    "        # print(f'embedded_shape {embedded_.shape}')\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs shape: [src_len, batch_size, enc_hid_dim * num_directions]\n",
    "        # hidden, cell shape: [num_layers * num_directions, batch_size, enc_hid_dim]\n",
    "        # print(f'encoder_hidden_shape {hidden.shape} | encoder_cell_shape {cell.shape} | outputs {outputs.shape}')\n",
    "\n",
    "        # Concatenate the hidden states from both directions\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\n",
    "        cell = torch.cat((cell[0], cell[1]), dim=-1)\n",
    "        # hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        # cell = torch.tanh(self.fc(torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)))\n",
    "        # print(f\"final_encoder_output_hidden {hidden.shape}\")\n",
    "        # print(f\"final_encoder_output_cell {cell.shape}\")\n",
    "        # print(\"***********************************************\")\n",
    "\n",
    "        # print(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim, dropout=dropout)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)  # input: [1, batch_size]\n",
    "        # print(f'input inside  decoder {input.shape}')\n",
    "        embedded = self.dropout(self.embedding(input))  # embedded: [1, batch_size, emb_dim]\n",
    "        # print(f'embedded inside decoder {embedded.shape}')\n",
    "        # Calculate attention weights\n",
    "        # print(hidden.shape, encoder_outputs.shape)\n",
    "        attention = self.attention(hidden, encoder_outputs)  # attention: [batch_size, src_len]\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        attention = attention.unsqueeze(1)  # attention: [batch_size, 1, src_len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs: [batch_size, src_len, enc_hid_dim * 2]\n",
    "        weighted = torch.bmm(attention, encoder_outputs)  # weighted: [batch_size, 1, enc_hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)  # weighted: [1, batch_size, enc_hid_dim * 2]\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input: [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        # Prepare input for fully connected layer\n",
    "        output = output.squeeze(0)  # output: [batch_size, dec_hid_dim]\n",
    "        embedded = embedded.squeeze(0)  # embedded: [batch_size, emb_dim]\n",
    "        weighted = weighted.squeeze(0)  # weighted: [batch_size, enc_hid_dim * 2]\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "\n",
    "        return prediction, hidden.squeeze(0), cell.squeeze(0), attention.squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "        # src_ = src.permute(1,0)\n",
    "        # trg_ = trg.permute(1,0)\n",
    "        # # print(f'src {src.shape} | src_ {src_.shape}')\n",
    "        # # print(f'trg {trg.shape} | trg_ {trg_.shape}')\n",
    "        # batch_size = trg_.shape[1]\n",
    "        # trg_len = trg_.shape[0]\n",
    "\n",
    "        # # print(f'batch_size {batch_size} | trg_len {trg_len}')\n",
    "        # trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.6):\n",
    "        src_ = src.permute(1,0)\n",
    "        trg_ = trg.permute(1,0)\n",
    "        # print(trg_)\n",
    "        batch_size = trg_.shape[1]\n",
    "        trg_len = trg_.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len-1, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_)\n",
    "\n",
    "        input = trg_[0,:]\n",
    "        # input = torch.tensor(2).unsqueeze(0).to(device)\n",
    "        # print(f'input outside loop {input}')\n",
    "\n",
    "        for t in range(0, trg_len-1):\n",
    "            # print(f'input seq to seq {input}')\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg_[t+1,:] if teacher_force else top1\n",
    "\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "ENC_EMB_DIM = 100  # Embedding dimension for encoder\n",
    "DEC_EMB_DIM = 100  # Embedding dimension for decoder\n",
    "ENC_HID_DIM = 256  # Hidden dimension for encoder\n",
    "DEC_HID_DIM = 512 # Hidden dimension for decoder \n",
    "ENC_DROPOUT = 0.5  # Dropout rate for encoder\n",
    "DEC_DROPOUT = 0.5  # Dropout rate for decoder\n",
    "OUTPUT_DIM = len(formula_vocab)  # Output dimension is the size of formula vocabulary\n",
    "\n",
    "# Instantiating the model components\n",
    "encoder = Encoder(len(problem_vocab), ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, problem_embedding_matrix)\n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Loss function, ignoring the index of the padding\n",
    "PAD_IDX = formula_vocab[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        input_ids = batch['problem'].to(device)\n",
    "\n",
    "        trg = batch['formula'].to(device)\n",
    "       \n",
    "        trg_input = trg\n",
    "        trg_output = trg[:, 1:]\n",
    "        # trg_new = trg[:, 1:]\n",
    "        # print(f'trg_out : {trg_output.shape}')\n",
    "        # print(f'trg_output : {trg_output}')\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        output = model(input_ids, trg)\n",
    "        # print(f'output {output.shape}')\n",
    "\n",
    "        # Reshape output and target tensors to compute loss\n",
    "        output_dim = output.shape[-1]\n",
    "        # output = output.contiguous().view(-1, output_dim)\n",
    "        # trg_output = trg_output.contiguous().view(-1)\n",
    "        output=output.permute(1,2,0)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg_output)\n",
    "        # print(loss)\n",
    "        loss.backward() \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Gradient clipping\n",
    "\n",
    "        optimizer.step() \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch['problem'].to(device)\n",
    "            trg = batch['formula'].to(device)\n",
    "            # print(src)\n",
    "\n",
    "            trg_new = trg[:, 1:]  \n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            # print(f'output shape {output.shape}')\n",
    "            # print(output)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.permute(1, 2, 0)  \n",
    "\n",
    "            loss = criterion(output, trg_new)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d31cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8326"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problem_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23951a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"attn_problem_vocab.json\", \"w\") as json_file:\n",
    "    json.dump(problem_vocab, json_file, indent=4)\n",
    "\n",
    "embedding_matrix_list = problem_embedding_matrix.tolist()\n",
    "with open(\"sttn_embedding_matrix.json\", \"w\") as json_file:\n",
    "    json.dump(embedding_matrix_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818b8dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formula_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683eb785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:23:00.974129Z",
     "iopub.status.busy": "2024-04-05T08:23:00.973736Z",
     "iopub.status.idle": "2024-04-05T09:50:05.605543Z",
     "shell.execute_reply": "2024-04-05T09:50:05.604300Z"
    },
    "papermill": {
     "duration": 5224.638177,
     "end_time": "2024-04-05T09:50:05.607878",
     "exception": false,
     "start_time": "2024-04-05T08:23:00.969701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.936 | Val Loss : 1.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Train Loss: 0.769 | Val Loss : 1.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Train Loss: 0.708 | Val Loss : 1.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Train Loss: 0.646 | Val Loss : 1.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Train Loss: 0.589 | Val Loss : 1.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Train Loss: 0.528 | Val Loss : 1.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Train Loss: 0.478 | Val Loss : 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Train Loss: 0.430 | Val Loss : 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Train Loss: 0.385 | Val Loss : 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 0.350 | Val Loss : 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 0.320 | Val Loss : 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 0.297 | Val Loss : 0.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 0.277 | Val Loss : 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 0.257 | Val Loss : 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 0.243 | Val Loss : 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 0.228 | Val Loss : 0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 0.215 | Val Loss : 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 0.207 | Val Loss : 0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 0.200 | Val Loss : 0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 0.192 | Val Loss : 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Train Loss: 0.182 | Val Loss : 0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Train Loss: 0.180 | Val Loss : 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Train Loss: 0.171 | Val Loss : 0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Train Loss: 0.165 | Val Loss : 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:45<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Train Loss: 0.159 | Val Loss : 0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Train Loss: 0.157 | Val Loss : 0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:44<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Train Loss: 0.152 | Val Loss : 0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Train Loss: 0.146 | Val Loss : 0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Train Loss: 0.143 | Val Loss : 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:46<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Train Loss: 0.137 | Val Loss : 0.653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "loss_data = {\n",
    "    'train_losses': train_losses,\n",
    "    'valid_losses': valid_losses\n",
    "}\n",
    "\n",
    "N_EPOCHS = 30\n",
    "CLIP = 1\n",
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    if epoch > 30:\n",
    "        learning_rate = 0.0001\n",
    "    train_loss = train(model, data_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_data_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss : {valid_loss:.3f}')\n",
    "\n",
    "    # Update the scheduler\n",
    "#     scheduler.step(valid_loss)\n",
    "\n",
    "   \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'best_valid_loss': best_valid_loss\n",
    "        }\n",
    "        torch.save(checkpoint, f'/kaggle/working/retraained_model_attention_0.6_checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), '/kaggle/working/retrained_attention_model_0.6.pt')\n",
    "\n",
    "# Save loss data\n",
    "with open('/kaggle/working/loss_data_model_attention_update_1.json', 'w') as f:\n",
    "    json.dump(loss_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7df26b",
   "metadata": {
    "papermill": {
     "duration": 2.465538,
     "end_time": "2024-04-05T09:50:10.621240",
     "exception": false,
     "start_time": "2024-04-05T09:50:08.155702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 715814,
     "sourceId": 1246668,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4705930,
     "sourceId": 7993376,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5280.949506,
   "end_time": "2024-04-05T09:50:15.205184",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-05T08:22:14.255678",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
